{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7vFhzmPm/xqyeubUVsTz/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Plant Disease Detection using Transfer Learning (MobileNetV2)\n",
        "\n",
        "## 1. Introduction\n",
        "In this notebook, we develop a Convolutional Neural Network (CNN) based on the **MobileNetV2** architecture to classify plant diseases. The objective is to achieve high accuracy while maintaining a low parameter count, making the model suitable for mobile edge computing.\n",
        "\n",
        "**Methodology:**\n",
        "* **Data Source:** PlantVillage Dataset (38 Classes).\n",
        "* **Technique:** Transfer Learning (Feature Extraction & Fine-Tuning).\n",
        "* **Framework:** TensorFlow/Keras."
      ],
      "metadata": {
        "id": "zAeW8t4uY35r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import splitfolders\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Configuration\n",
        "# Setting seeds for reproducibility in academic research\n",
        "SEED = 42\n",
        "tf.random.set_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")\n",
        "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
      ],
      "metadata": {
        "id": "tqM68ROcY5bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Data Preparation and Engineering\n",
        "We utilize the `split-folders` library to physically partition the dataset into Training, Validation, and Test sets. This prevents **data leakage**, a common issue in deep learning projects where validation data inadvertently bleeds into the training process."
      ],
      "metadata": {
        "id": "G4EN5gOmY97x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Path Configuration\n",
        "# NOTE: Ensure you have downloaded the dataset to 'data/raw/PlantVillage'\n",
        "INPUT_FOLDER = '../data/raw/PlantVillage'\n",
        "OUTPUT_FOLDER = '../data/processed/PlantVillage_Split'\n",
        "\n",
        "def split_dataset(input_path, output_path):\n",
        "    \"\"\"\n",
        "    Splits the dataset into training, validation, and testing sets.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): Path to the raw dataset.\n",
        "        output_path (str): Path where the split dataset will be saved.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(input_path):\n",
        "        print(f\"Dataset not found at {input_path}. Please download it first.\")\n",
        "        return\n",
        "\n",
        "    print(\"Splitting dataset... This may take a while.\")\n",
        "    # Split ratio: 80% Train, 10% Validation, 10% Test\n",
        "    splitfolders.ratio(input_path, output=output_path,\n",
        "                       seed=SEED, ratio=(.8, .1, .1),\n",
        "                       group_prefix=None, move=False)\n",
        "    print(\"Data splitting completed successfully.\")\n",
        "\n",
        "# Uncomment the line below to run splitting (Run once)\n",
        "# split_dataset(INPUT_FOLDER, OUTPUT_FOLDER)"
      ],
      "metadata": {
        "id": "4UTk6tttZAfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Image Parameters\n",
        "IMG_SIZE = (224, 224)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "DATA_DIR = '../data/processed/PlantVillage_Split'\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
        "VAL_DIR = os.path.join(DATA_DIR, 'val')\n",
        "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
        "\n",
        "# Using tf.keras.utils.image_dataset_from_directory for efficient data pipeline\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    labels='categorical',\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    VAL_DIR,\n",
        "    labels='categorical',\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    TEST_DIR,\n",
        "    labels='categorical',\n",
        "    image_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Retrieve class names\n",
        "class_names = train_ds.class_names\n",
        "print(f\"Number of Classes: {len(class_names)}\")\n",
        "print(f\"Example Classes: {class_names[:5]}\")\n",
        "\n",
        "# Performance Optimization: Prefetching and Caching\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "k0hpj6S3ZEdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Model Architecture: MobileNetV2\n",
        "We employ **MobileNetV2** as the backbone. The weights are pre-trained on ImageNet.\n",
        "1.  **Preprocessing:** Inputs are scaled to `[-1, 1]`.\n",
        "2.  **Base:** MobileNetV2 (frozen).\n",
        "3.  **Head:** Global Average Pooling -> Dropout -> Dense (Softmax)."
      ],
      "metadata": {
        "id": "x1fV1aR_ZGPl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(num_classes):\n",
        "    \"\"\"\n",
        "    Constructs the Transfer Learning model based on MobileNetV2.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): Number of target classes (38 for PlantVillage).\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Compiled model ready for training.\n",
        "    \"\"\"\n",
        "    # 1. Base Model\n",
        "    base_model = tf.keras.applications.MobileNetV2(\n",
        "        input_shape=IMG_SIZE + (3,),\n",
        "        include_top=False,\n",
        "        weights='imagenet'\n",
        "    )\n",
        "\n",
        "    # Freeze the base model to prevent destroying learned features\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # 2. Input and Preprocessing\n",
        "    inputs = tf.keras.Input(shape=IMG_SIZE + (3,))\n",
        "    x = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "\n",
        "    # 3. Feature Extraction\n",
        "    x = base_model(x, training=False)\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dropout(0.2)(x)  # Regularization to prevent overfitting\n",
        "\n",
        "    # 4. Classification Head\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "model = build_model(len(class_names))\n",
        "model.summary()\n",
        "\n",
        "# Compilation\n",
        "model.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "wzp_uf_sZJ_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training Process\n",
        "We implement callbacks such as `EarlyStopping` and `ModelCheckpoint` to ensure the training process is efficient and saves the best performing weights."
      ],
      "metadata": {
        "id": "P_qK089QZLe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "# Callbacks\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"plant_village_mobilenetv2.h5\",\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=5,\n",
        "    restore_best_weights=True,\n",
        "    monitor='val_loss'\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb]\n",
        ")"
      ],
      "metadata": {
        "id": "EtjgwK36ZNBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history):\n",
        "    \"\"\"Plots training and validation metrics.\"\"\"\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history)"
      ],
      "metadata": {
        "id": "_nbCOTP0ZOrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Model Evaluation\n",
        "The final evaluation is conducted on the **Test Set**, which the model has never seen during training. This provides an unbiased estimate of the model's performance in real-world scenarios."
      ],
      "metadata": {
        "id": "7QZeUUW-ZQJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on Test Data\n",
        "loss, accuracy = model.evaluate(test_ds)\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Confusion Matrix & Classification Report\n",
        "# Note: This requires extracting labels from the dataset\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in test_ds:\n",
        "    preds = model.predict(images, verbose=0)\n",
        "    y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
        "    y_pred.extend(np.argmax(preds, axis=1))\n",
        "\n",
        "print(classification_report(y_true, y_pred, target_names=class_names))"
      ],
      "metadata": {
        "id": "1f_7zGayZRxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# [CELL 13] - Explainable AI (XAI): Grad-CAM Implementation\n",
        "# ---------------------------------------------------------\n",
        "# This module visualizes the \"Region of Interest\" (ROI) that the model focuses on.\n",
        "# It uses Gradient-weighted Class Activation Mapping (Grad-CAM).\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_img_array(img_path, size):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses an image for the model.\n",
        "    Note: Since our model includes a preprocessing layer, we return the raw image array\n",
        "    but expanded to match batch dimensions.\n",
        "    \"\"\"\n",
        "    # Load image with target size\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=size)\n",
        "    # Convert to array\n",
        "    array = tf.keras.preprocessing.image.img_to_array(img)\n",
        "    # Add batch dimension (1, 224, 224, 3)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    \"\"\"\n",
        "    Generates a Grad-CAM heatmap by accessing the internal layers of MobileNetV2.\n",
        "    \"\"\"\n",
        "    # 1. Access the internal MobileNetV2 base layer dynamically\n",
        "    base_model_layer = None\n",
        "    for layer in model.layers:\n",
        "        # Check if the layer is a Functional model (MobileNetV2)\n",
        "        if isinstance(layer, tf.keras.Model):\n",
        "            base_model_layer = layer\n",
        "            break\n",
        "\n",
        "    if base_model_layer is None:\n",
        "        # Fallback: If model is not nested, assume 'model' is the base\n",
        "        base_model_layer = model\n",
        "\n",
        "    # 2. Access the last convolutional layer ('out_relu' is standard for MobileNetV2)\n",
        "    try:\n",
        "        last_conv_layer = base_model_layer.get_layer(last_conv_layer_name)\n",
        "    except ValueError:\n",
        "        raise ValueError(f\"Layer '{last_conv_layer_name}' not found in the model.\")\n",
        "\n",
        "    # 3. Create a sub-model that outputs the last conv layer\n",
        "    # We map the inputs of the base model to the output of the last conv layer\n",
        "    last_conv_layer_model = tf.keras.Model(base_model_layer.inputs, last_conv_layer.output)\n",
        "\n",
        "    # 4. Create a classifier model\n",
        "    # It takes the output of the last conv layer and passes it through the rest of the main model\n",
        "    # (GAP + Dropout + Dense)\n",
        "    classifier_input = tf.keras.Input(shape=last_conv_layer.output.shape[1:])\n",
        "    x = classifier_input\n",
        "\n",
        "    # We need to find where the base model ends in the main model's layer list\n",
        "    # and pass the input through the remaining layers (GAP, Dropout, Dense)\n",
        "    # Typically, these are the last 3 layers in our architecture.\n",
        "    for layer in model.layers[-3:]:\n",
        "        x = layer(x)\n",
        "\n",
        "    classifier_model = tf.keras.Model(classifier_input, x)\n",
        "\n",
        "    # 5. Compute Gradients\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Preprocess input manually since we are bypassing the main model's preprocessing layer\n",
        "        # MobileNetV2 expects [-1, 1] scaling.\n",
        "        inputs = tf.keras.applications.mobilenet_v2.preprocess_input(tf.cast(img_array, tf.float32))\n",
        "\n",
        "        # Get conv output\n",
        "        last_conv_layer_output = last_conv_layer_model(inputs)\n",
        "        tape.watch(last_conv_layer_output)\n",
        "\n",
        "        # Get predictions\n",
        "        preds = classifier_model(last_conv_layer_output)\n",
        "\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # 6. Gradient calculation and pooling\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # 7. Generate Heatmap\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # 8. Normalize\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "def save_and_display_gradcam(img_path, heatmap, alpha=0.4):\n",
        "    \"\"\"\n",
        "    Superimposes the heatmap on the original image and displays it using Matplotlib.\n",
        "    \"\"\"\n",
        "    # Load original image\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (224, 224))\n",
        "\n",
        "    # Rescale heatmap to 0-255\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # Colorize heatmap\n",
        "    jet = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "    # Superimpose\n",
        "    superimposed_img = jet * alpha + img\n",
        "    superimposed_img = np.clip(superimposed_img, 0, 255).astype('uint8')\n",
        "\n",
        "    # Display\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    superimposed_rgb = cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(img_rgb)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(superimposed_rgb)\n",
        "    plt.title(\"Grad-CAM Attention Map\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# --- EXECUTION BLOCK ---\n",
        "try:\n",
        "    # 1. Define the Test Directory explicitly to avoid 'test_ds' dependency issues\n",
        "    # Ensure this path matches the OUTPUT_FOLDER defined in Cell 4\n",
        "    TEST_DIR_PATH = '../data/processed/PlantVillage_Split/test'\n",
        "\n",
        "    if not os.path.exists(TEST_DIR_PATH):\n",
        "        print(f\"Error: Test directory not found at {TEST_DIR_PATH}\")\n",
        "    else:\n",
        "        # 2. Collect all image paths from the test directory\n",
        "        all_test_images = []\n",
        "        for root, dirs, files in os.walk(TEST_DIR_PATH):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    all_test_images.append(os.path.join(root, file))\n",
        "\n",
        "        if len(all_test_images) > 0:\n",
        "            # 3. Select a random image\n",
        "            random_img_path = random.choice(all_test_images)\n",
        "\n",
        "            # Extract class name from path for display\n",
        "            actual_class = os.path.basename(os.path.dirname(random_img_path))\n",
        "            print(f\"Analyzing Image: {random_img_path}\")\n",
        "            print(f\"Actual Class: {actual_class}\")\n",
        "\n",
        "            # 4. Prepare image\n",
        "            img_array = get_img_array(random_img_path, size=(224, 224))\n",
        "\n",
        "            # 5. Generate Heatmap\n",
        "            # 'out_relu' is the standard last conv layer for MobileNetV2\n",
        "            heatmap = make_gradcam_heatmap(img_array, model, 'out_relu')\n",
        "\n",
        "            # 6. Display Results\n",
        "            save_and_display_gradcam(random_img_path, heatmap)\n",
        "        else:\n",
        "            print(\"No images found in the test directory.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"XAI Error: {e}\")\n",
        "    print(\"Ensure that the model is compiled and the 'out_relu' layer exists.\")"
      ],
      "metadata": {
        "id": "ezcqRLlUf6cC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}